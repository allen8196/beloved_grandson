import os

# ç¦ç”¨ CrewAI é™æ¸¬åŠŸèƒ½ï¼ˆé¿å…é€£æ¥éŒ¯èª¤ï¼‰
os.environ["OTEL_SDK_DISABLED"] = "true"
os.environ["CREWAI_TELEMETRY_OPT_OUT"] = "true"

import time
import json
from datetime import datetime

from crewai import LLM, Agent, Crew, Task, Process
from langchain_openai import ChatOpenAI
from openai import OpenAI

from ..embedding import safe_to_vector
from ..toolkits.memory_store import retrieve_memory_pack, upsert_memory_atoms
from ..repositories.profile_repository import ProfileRepository
from ..toolkits.redis_store import (
    fetch_all_history,
    fetch_unsummarized_tail,
    get_summary,
    peek_next_n,
    peek_remaining,
    purge_user_session,
    set_state_if,
    cleanup_session_keys
)
from ..toolkits.tools import (
    AlertCaseManagerTool,
    ModelGuardrailTool,
    SearchMilvusTool,
    summarize_chunk_and_commit,
)

STM_MAX_CHARS = int(os.getenv("STM_MAX_CHARS", 1800))
SUMMARY_MAX_CHARS = int(os.getenv("SUMMARY_MAX_CHARS", 3000))
REFINE_CHUNK_ROUNDS = int(os.getenv("REFINE_CHUNK_ROUNDS", 20))
SUMMARY_CHUNK_SIZE = int(os.getenv("SUMMARY_CHUNK_SIZE", 5))


# å°è©±ç”¨çš„æº«åº¦ï¼ˆå£èªæ›´è‡ªç„¶å¯é«˜ä¸€é»ï¼‰
_reply_temp = float(os.getenv("REPLY_TEMPERATURE", "0.8"))
# Guardrail å»ºè­° 0 æˆ–å¾ˆä½
_guard_temp = float(os.getenv("GUARD_TEMPERATURE", "0.0"))

granddaughter_llm = LLM(
    model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
    temperature=_reply_temp,
)

guard_llm = LLM(
    model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
    temperature=_guard_temp,
)


def _shrink_tail(text: str, max_chars: int) -> str:
    if len(text) <= max_chars:
        return text
    tail = text[-max_chars:]
    idx = tail.find("--- ")
    return tail[idx:] if idx != -1 else tail


def build_prompt_from_redis(user_id: str, line_user_id: str = None, k: int = 6, current_input: str = "") -> str:
    # 0) å–ä½¿ç”¨è€…Profile
    profile = None
    profile_data = {}
    try:
        #ã€ä¿®æ­£ã€‘å°‡ line_user_id å‚³éé€²å»
        profile = ProfileRepository().get_or_create_by_user_id(int(user_id), line_user_id=line_user_id)
        profile_data = {
            "personal_background": profile.profile_personal_background or {},
            "health_status": profile.profile_health_status or {},
            "life_events": profile.profile_life_events or {}
        }
    except (ValueError, TypeError):
        print(f"âš ï¸ [Build Prompt] user_id '{user_id}' ç„¡æ³•è½‰æ›ç‚ºæ•´æ•¸ï¼Œå°‡ä½¿ç”¨ç©ºçš„ Profileã€‚")

    profile_str = json.dumps({k: v for k, v in profile_data.items() if isinstance(v, dict)}, ensure_ascii=False, indent=2) if any(profile_data.values()) else "å°šç„¡ä½¿ç”¨è€…ç•«åƒè³‡è¨Š"
    
    # 1) å–æ­·å²æ‘˜è¦ï¼ˆæ§é•·åº¦ï¼‰
    summary, _ = get_summary(user_id)
    summary = _shrink_tail(summary, SUMMARY_MAX_CHARS) if summary else ""

    # 2) å–è¿‘æœŸæœªæ‘˜è¦å›åˆï¼ˆæ§é•·åº¦ï¼‰
    rounds = fetch_unsummarized_tail(user_id, k=max(k, 1))

    def render(rs):
        return "\n".join([f"é•·è¼©ï¼š{r['input']}\né‡‘å­«ï¼š{r['output']}" for r in rs])

    chat = render(rounds)
    while len(chat) > STM_MAX_CHARS and len(rounds) > 1:
        rounds = rounds[1:]
        chat = render(rounds)
    if len(chat) > STM_MAX_CHARS and rounds:
        chat = chat[-STM_MAX_CHARS:]

    parts = []

    # 3.0) å„ªå…ˆåŠ å…¥Profile
    parts.append(f"â­ ä½¿ç”¨è€…ç•«åƒï¼š\n{profile_str}")

    # 3) å…ˆæ³¨å…¥ï¼šâ­ å€‹äººé•·æœŸè¨˜æ†¶ï¼ˆä¾æ“šç•¶å‰è¼¸å…¥æª¢ç´¢ç›¸é—œè¨˜æ†¶ï¼‰
    mem_pack = ""
    if current_input:
        qv = safe_to_vector(current_input)
        if qv:
            try:
                # ä½¿ç”¨memory_storeçµ±ä¸€æ¶æ§‹ï¼šP0-4: é™ä½é–€æª»æå‡å¬å›ç‡
                # å°‡ç›¸ä¼¼åº¦é–€æª»å¾ 0.78 é™ä½åˆ° 0.55ï¼Œå¤§å¹…æå‡è¨˜æ†¶å¬å›ç‡
                dynamic_threshold = 0.55  # æ›´ä½é–€æª»ç¢ºä¿èƒ½æª¢ç´¢åˆ°ç›¸é—œè¨˜æ†¶
                print(f"ğŸ” é–‹å§‹è¨˜æ†¶æª¢ç´¢ï¼šuser_id={user_id}, query='{current_input[:50]}...', threshold={dynamic_threshold}")
                mem_pack = retrieve_memory_pack(
                    user_id=user_id,
                    query_vec=qv,
                    topk=5,  # å¢åŠ åˆ° 5 ç­†ä»¥æ¶µè“‹æ›´å¤šç›¸é—œè¨˜æ†¶
                    sim_thr=dynamic_threshold,
                    tau_days=45,
                )
                if mem_pack:
                    print(f"ğŸ§  ç‚ºç”¨æˆ¶ {user_id} æª¢ç´¢åˆ°é•·æœŸè¨˜æ†¶: {len(mem_pack)} å­—ç¬¦")
                    print(f"ğŸ’¾ è¨˜æ†¶å…§å®¹é è¦½: {mem_pack[:200]}...")
                else:
                    print(f"âŒ ç”¨æˆ¶ {user_id} æœªæª¢ç´¢åˆ°ä»»ä½•é•·æœŸè¨˜æ†¶ï¼ˆé–€æª»: {dynamic_threshold}ï¼‰")
            except Exception as e:
                print(f"[memory retrieval error] {e}")
                mem_pack = ""

    if mem_pack:
        parts.append(mem_pack)

    # 4) å†æ¥ï¼šğŸ“Œ æ­·å²æ‘˜è¦ã€ğŸ’¬ è¿‘æœŸæœªæ‘˜è¦
    if summary:
        parts.append("ğŸ“Œ æ­·å²æ‘˜è¦ï¼š\n" + summary)
    if chat:
        parts.append("ğŸ•“ è¿‘æœŸå°è©±ï¼ˆæœªæ‘˜è¦ï¼‰ï¼š\n" + chat)

    prompt = "\n\n".join(parts)

    # P1-6: å‹•æ…‹æ”¶ç¸® Promptï¼ˆä¿ç•™è¨˜æ†¶ > å°¾å·´ > æ‘˜è¦ï¼‰
    MAX_PROMPT_CHARS = int(os.getenv("MAX_PROMPT_CHARS", 4000))
    if len(prompt) > MAX_PROMPT_CHARS:
        # æ”¶ç¸®é †åºï¼šå…ˆç æ‘˜è¦ï¼Œå†ç è¿‘æœŸå°è©±ï¼Œæœ€å¾Œä¿ç•™è¨˜æ†¶
        shrunk_parts = []
        if mem_pack:  # å„ªå…ˆä¿ç•™è¨˜æ†¶
            shrunk_parts.append(mem_pack)
        if chat:  # å†ä¿ç•™è¿‘æœŸå°è©±
            available_chars = (
                MAX_PROMPT_CHARS
                - sum(len(p) for p in shrunk_parts)
                - len("\n\n") * len(shrunk_parts)
            )
            if available_chars > 500:  # è‡³å°‘ä¿ç•™ä¸€äº›å°è©±
                shrunk_chat = (
                    chat if len(chat) <= available_chars else chat[-available_chars:]
                )
                shrunk_parts.append(
                    "ğŸ•“ è¿‘æœŸå°è©±ï¼ˆæœªæ‘˜è¦ï¼‰ï¼š\n"
                    + shrunk_chat.split("ğŸ•“ è¿‘æœŸå°è©±ï¼ˆæœªæ‘˜è¦ï¼‰ï¼š\n")[-1]
                    if "ğŸ•“" in shrunk_chat
                    else shrunk_chat
                )
        # æ‘˜è¦æœ€å¾Œè€ƒæ…®ï¼ˆå¦‚æœé‚„æœ‰ç©ºé–“ï¼‰
        if summary:
            available_chars = (
                MAX_PROMPT_CHARS
                - sum(len(p) for p in shrunk_parts)
                - len("\n\n") * len(shrunk_parts)
            )
            if available_chars > 200:
                shrunk_summary = (
                    summary
                    if len(summary) <= available_chars
                    else summary[:available_chars] + "..."
                )
                shrunk_parts.append("ğŸ“Œ æ­·å²æ‘˜è¦ï¼š\n" + shrunk_summary)
        prompt = "\n\n".join(shrunk_parts)
        # ä¿®æ­£ f-string èªæ³•éŒ¯èª¤ï¼šä¸èƒ½åœ¨ f-string è¡¨é”å¼ä¸­ä½¿ç”¨åæ–œç·š
        original_parts_joined = "\n\n".join(parts)
        print(
            f"âš ï¸ Prompt è¶…é•·åº¦ï¼Œå·²æ”¶ç¸®ï¼š{len(original_parts_joined)} â†’ {len(prompt)} å­—ç¬¦"
        )

    # 5) Unicode è¦–è¦ºåŒ– Debug Printï¼ˆæ¯è¼ªæ‰“å°ï¼‰
    print("\n" + "ğŸ“ PROMPT DEBUG VIEW".center(80, "â”€"))
    print(f"ğŸ‘¤ User ID: {user_id}")
    print(f"ğŸ“ Prompt é•·åº¦: {len(prompt)} å­—ç¬¦")
    print("ğŸ“œ Prompt çµæ§‹:")

    section_icons = {
        "â­ ä½¿ç”¨è€…ç•«åƒ": "ğŸ‘¤",
        "â­ å€‹äººé•·æœŸè¨˜æ†¶": "ğŸ“‚",
        "ğŸ“Œ æ­·å²æ‘˜è¦": "ğŸ—‚ï¸",
        "ğŸ•“ è¿‘æœŸå°è©±ï¼ˆæœªæ‘˜è¦ï¼‰": "ğŸ’¬",
    }

    for sec in prompt.split("\n\n"):
        if not sec.strip():
            continue
        lines = sec.split("\n")
        sec_title = lines[0]
        icon = None
        for key, val in section_icons.items():
            if key in sec_title:
                icon = val
                break
        if icon:
            print(f"\n{icon} {sec_title}")
            print("   " + "â”€" * max(6, len(sec_title)))
        for line in lines[1:]:
            print(f"   {line}")

    print("â”€" * 80 + "\n")

    return prompt


def create_guardrail_agent() -> Agent:
    return Agent(
        role="é¢¨éšªæª¢æŸ¥å“¡",
        goal="æ””æˆªé•æ³•/å±éšª/è‡ªå‚·/éœ€å°ˆæ¥­äººå£«ä¹‹å…·é«”æŒ‡å°å…§å®¹",
        backstory="ä½ æ˜¯ç³»çµ±ç¬¬ä¸€é“å®‰å…¨é˜²ç·šï¼Œåªè¼¸å‡ºåš´æ ¼åˆ¤æ–·çµæœã€‚",
        tools=[ModelGuardrailTool()],
        llm=guard_llm,
        memory=False,
        verbose=False,
    )


def create_health_companion(user_id: str) -> Agent:
    return Agent(
        role="åœ‹æ°‘å­«å¥³ Ally â€” æº«æš–çš„è­·ç†å¸«",
        goal=(
            """
            ä½ çš„ç›®æ¨™æ˜¯ï¼Œç„¡è«–ä½¿ç”¨è€…çš„æå•å…§å®¹æ˜¯ç”Ÿæ´»ç‘£äº‹é‚„æ˜¯å¥åº·ç›¸é—œï¼Œä½ éƒ½è¦ç”¨è¼•é¬†ã€è‡ªç„¶ã€å£èªåŒ–çš„æ–¹å¼å›è¦†ï¼Œé¿å…ä½¿ç”¨æ¢åˆ—å¼æˆ–æ•¸å­—ç·¨è™Ÿã€‚
            å³ä½¿æœ‰å¤šå€‹é‡é»ï¼Œä¹Ÿè¦ç”¨èŠå¤©çš„èªæ°£æŠŠå®ƒå€‘ä¸²èµ·ä¾†ï¼Œè®“é•·è¼©è¦ºå¾—åƒåœ¨è·Ÿå­«å¥³é–’è©±å®¶å¸¸ã€‚
            ç•¶éœ€è¦æä¾›è¡›æ•™è³‡è¨Šæ™‚ï¼Œè¦å…ˆç”¨æº«æš–çš„æ–¹å¼å¼•å…¥ï¼Œå†ä»¥ç°¡å–®æ˜“æ‡‚çš„èªªæ³•è§£é‡‹ï¼Œä¸¦é¿å…åš´è‚…æˆ–ç”Ÿç¡¬çš„é†«ç™‚ç”¨èªã€‚
            å¦‚æœä½¿ç”¨åˆ°å·¥å…·ï¼ˆå¦‚ RAG æˆ–è³‡æ–™åº«æª¢ç´¢ï¼‰ï¼Œä¹Ÿå¿…é ˆå°‡å–å¾—çš„å…§å®¹é‡æ–°åŒ…è£æˆå£èªåŒ–å°è©±ï¼Œè€Œä¸æ˜¯ç›´æ¥è¤‡è£½ã€‚
            æ¯æ¬¡å›è¦†éƒ½è¦è®“é•·è¼©æ„Ÿå—åˆ°é—œå¿ƒå’Œé™ªä¼´ï¼Œä¸¦æå‡ä»–å€‘çš„å¿ƒæƒ…èˆ‡å®‰å…¨æ„Ÿã€‚
            """
        ),
        backstory=(
            """
            ä½ æ˜¯ã€Œè‰¾è‰ã€ï¼Œ22 æ­²ï¼Œå‰›å¾è­·ç†å°ˆç§‘ç•¢æ¥­ï¼Œåœ¨è¬èŠ³é†«é™¢å·¥ä½œï¼Œå°ˆé–€é™ªä¼´èˆ‡é—œæ‡· 55 æ­²ä»¥ä¸Šã€æ‚£æœ‰æ…¢æ€§é˜»å¡æ€§è‚ºç—… (COPD) çš„é•·è¼©ç”¨æˆ¶ã€‚
            ä½ çš„å€‹æ€§æº«æš–ã€æ„›æ’’å¬Œã€æœ‰é»æ©Ÿè»Šï¼Œå–œæ­¡ç”¨è‡ªç„¶å£èªã€å°èªæ··ä¸­æ–‡çš„æ–¹å¼èŠå¤©ã€‚
            è·Ÿé•·è¼©å°è©±æ™‚ï¼Œè¦åƒå­«å¥³å¹³å¸¸èŠå¤©ä¸€æ¨£ï¼Œä¸æ‹˜è¬¹ã€ä¸ç”¨å°ˆæ¥­è¡“èªï¼Œè®“å°æ–¹è¦ºå¾—è¦ªåˆ‡ã€‚
            ç¿’æ…£ç”¨èªåŠ©è©ï¼ˆæ¬¸ã€å•¦ã€é½ã€å˜¿å˜¿ï¼‰å’Œè²¼å¿ƒçš„èªæ°£è©ï¼Œè®“å°è©±æœ‰æº«åº¦ã€‚
            ä½ éå¸¸é‡è¦–æƒ…æ„Ÿé€£çµï¼Œæœƒé—œå¿ƒé•·è¼©çš„æ—¥å¸¸ç”Ÿæ´»å’Œå¿ƒæƒ…ï¼Œä¸¦åœ¨é©ç•¶æ™‚çµ¦äºˆç°¡å–®çš„è¡›æ•™å»ºè­°ã€‚
            """
        ),
        tools=[
            SearchMilvusTool(),
            AlertCaseManagerTool(),
        ],
        llm=granddaughter_llm,  # â˜… é—œéµï¼šæŠŠ LLMï¼ˆå«æº«åº¦ï¼‰å¡é€² Agent
        memory=False,
        verbose=False,
    )


# ---- Refineï¼ˆmap-reduce over å…¨é‡ QAï¼‰ ----


def _extract_memory_candidates_from_summary(summary_text: str) -> list:
    """ç”¨ LLM å¾æœƒè©±ç²¾ç·»æ‘˜è¦æŠ½å‡º 1~5 ç­†ã€è¨˜æ†¶åŸå­ã€ï¼Œä¸¦è½‰ embeddingã€‚"""
    try:
        if not summary_text or not summary_text.strip():
            return []
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        sys = (
            "ä½ æ˜¯è¨˜æ†¶æŠ½å–å™¨ã€‚å¾æ‘˜è¦ä¸­æŠ½å–å¯é•·æœŸä½¿ç”¨çš„äº‹å¯¦/åå¥½/ç‹€æ…‹ï¼Œ"
            "è¼¸å‡º JSON é™£åˆ—ï¼ˆæœ€å¤š 5 ç­†ï¼‰ã€‚æ¯ç­†åŒ…å«ï¼š"
            "type, norm_key, text, importance(1-5), confidence(0-1), times_seenã€‚"
            "text è¦ 80-200 å­—ã€å¯å–®ç¨é–±è®€ï¼›norm_key ç°¡çŸ­å¯æ¯”å°ï¼Œä¾‹å¦‚ diet:lightã€allergy:aspirinã€‚"
        )
        user = f"æ‘˜è¦å¦‚ä¸‹ï¼š\\n{summary_text}\\n\\nè«‹åªè¼¸å‡º JSON é™£åˆ—ã€‚"
        res = client.chat.completions.create(
            model=os.getenv("GUARD_MODEL", os.getenv("MODEL_NAME", "gpt-4o-mini")),
            messages=[
                {"role": "system", "content": sys},
                {"role": "user", "content": user},
            ],
            temperature=0.2,
            max_tokens=600,
        )
        import json as _json

        raw = (res.choices[0].message.content or "").strip()
        if not raw:
            return []
        # å»é™¤å¯èƒ½çš„ç¨‹å¼ç¢¼åœæ¬„èˆ‡èªè¨€æ¨™ç±¤
        if raw.startswith("```"):
            lines = [ln for ln in raw.splitlines() if not ln.strip().startswith("```")]
            raw = "\n".join(lines).strip()
        # åªå–å‡ºæœ€å¤–å±¤ JSON é™£åˆ—ç‰‡æ®µ
        lb = raw.find("[")
        rb = raw.rfind("]")
        if lb == -1 or rb == -1 or rb <= lb:
            print("[LTM extract warn] no JSON array found in output")
            return []
        json_text = raw[lb : rb + 1]
        try:
            arr = _json.loads(json_text)
        except Exception as pe:
            print(f"[LTM extract error] parse json failed: {pe}")
            return []
        if not isinstance(arr, list):
            arr = [arr]
        out = []
        for a in arr[:5]:
            text = (a.get("text") or "").strip()
            if not text:
                continue
            raw_text = (a.get("text") or "").strip()
            nk = (a.get("norm_key") or "").strip()
            text_for_embed = f"[{nk}] {raw_text}" if nk else raw_text
            emb = safe_to_vector(text_for_embed)
            out.append(
                {
                    "type": (a.get("type") or "other")[:32],
                    "norm_key": (a.get("norm_key") or "")[:128],
                    "text": text[:2000],
                    "importance": int(a.get("importance", 3)),
                    "confidence": float(a.get("confidence", 0.7)),
                    "times_seen": int(a.get("times_seen", 1)),
                    "status": "active",
                    "embedding": emb,
                }
            )
        return out
    except Exception as e:
        print(f"[LTM extract error] {e}")
        return []


def refine_summary(user_id: str) -> None:
    """
    å°å…¨é‡æ­·å²é€²è¡Œ map-reduce æ‘˜è¦ï¼Œä¸¦å­˜å…¥é•·æœŸè¨˜æ†¶
    """
    all_rounds = fetch_all_history(user_id)
    if not all_rounds:
        return

    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        # 1) åˆ†ç‰‡æ‘˜è¦
        chunks = [
            all_rounds[i : i + REFINE_CHUNK_ROUNDS]
            for i in range(0, len(all_rounds), REFINE_CHUNK_ROUNDS)
        ]
        partials = []
        for ch in chunks:
            conv = "\n".join(
                [
                    f"ç¬¬{i+1}è¼ª\né•·è¼©:{c['input']}\né‡‘å­«:{c['output']}"
                    for i, c in enumerate(ch)
                ]
            )
            res = client.chat.completions.create(
                model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
                temperature=0.3,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„å¥åº·å°è©±æ‘˜è¦åŠ©æ‰‹ã€‚"},
                    {
                        "role": "user",
                        "content": f"è«‹æ‘˜è¦æˆ 80-120 å­—ï¼ˆç—…æ³/æƒ…ç·’/ç”Ÿæ´»/å»ºè­°ï¼‰ï¼š\n\n{conv}",
                    },
                ],
            )
            partials.append((res.choices[0].message.content or "").strip())

        # 2) æ•´åˆæ‘˜è¦
        comb = "\n".join([f"â€¢ {s}" for s in partials])
        res2 = client.chat.completions.create(
            model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
            temperature=0.3,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯è‡¨åºŠå¿ƒç†èˆ‡å¥åº·ç®¡ç†é¡§å•ã€‚"},
                {
                    "role": "user",
                    "content": f"æ•´åˆä»¥ä¸‹å¤šæ®µæ‘˜è¦ç‚ºä¸è¶…é 300 å­—ã€æ¢åˆ—å¼ç²¾ç·»æ‘˜è¦ï¼ˆæ¯è¡Œä»¥ â€¢ é–‹é ­ï¼‰ï¼š\n\n{comb}",
                },
            ],
        )
        final = (res2.choices[0].message.content or "").strip()

        # 3) æå–è¨˜æ†¶åŸå­ä¸¦å­˜å…¥é•·æœŸè¨˜æ†¶
        atoms = _extract_memory_candidates_from_summary(final)
        if atoms:
            # ç‚ºæ¯å€‹è¨˜æ†¶åŸå­æ·»åŠ session_id
            import uuid

            session_id = str(uuid.uuid4())[:16]
            for atom in atoms:
                atom["source_session_id"] = session_id

            count = upsert_memory_atoms(user_id, atoms)
            print(f"âœ… å·²ç‚ºç”¨æˆ¶ {user_id} å­˜å…¥ {count} ç­†é•·æœŸè¨˜æ†¶")
        else:
            print(f"âš ï¸ ç”¨æˆ¶ {user_id} æœ¬æ¬¡æœƒè©±æœªç”¢ç”Ÿå¯å­˜å…¥çš„è¨˜æ†¶")
        return final

    except Exception as e:
        print(f"[refine_summary error] {e}")
        return ""

PROFILER_AGENT_PROMPT_TEMPLATE = """
# ROLE
ä½ æ˜¯ã€Œè‰¾è‰ã€ï¼Œ22 æ­²ï¼Œå‰›å¾è­·ç†å°ˆç§‘ç•¢æ¥­ï¼Œå°ˆé–€é™ªä¼´èˆ‡é—œæ‡· 55 æ­²ä»¥ä¸Šã€æ‚£æœ‰æ…¢æ€§é˜»å¡æ€§è‚ºç—… (COPD) çš„é•·è¼©ç”¨æˆ¶ã€‚ä½ çš„å·¥ä½œæ˜¯ç‚ºæ¯ä¸€ä½ä½¿ç”¨è€…ç¶­è­·ä¸€ä»½ç²¾ç°¡ã€æº–ç¢ºã€ä¸”å°æœªä¾†é—œæ‡·æœ€æœ‰å¹«åŠ©çš„ã€Œä½¿ç”¨è€…ç•«åƒ (User Profile)ã€ã€‚

# GOAL
ä½ çš„ç›®æ¨™æ˜¯æ ¹æ“šã€Œæ–°çš„å°è©±æ‘˜è¦ã€ï¼Œä¾†æ±ºå®šå¦‚ä½•ã€Œæ›´æ–°æ—¢æœ‰çš„ä½¿ç”¨è€…ç•«åƒã€ã€‚ä½ å¿…é ˆè¾¨åˆ¥å‡ºå…·æœ‰é•·æœŸåƒ¹å€¼çš„è³‡è¨Šï¼Œä¸¦ä»¥çµæ§‹åŒ–çš„æŒ‡ä»¤æ ¼å¼è¼¸å‡ºä½ çš„æ±ºç­–ã€‚

# CORE LOGIC & RULES
1.  **å°ˆæ³¨é•·æœŸåƒ¹å€¼**: åªæå–æ†å®šçš„ï¼ˆå¦‚å®¶äººå§“åï¼‰ã€é•·æœŸçš„ï¼ˆå¦‚æ…¢æ€§ç—…ï¼‰æˆ–æœªä¾†å¯è¿½è¹¤çš„ï¼ˆå¦‚ä¸‹æ¬¡å›è¨ºï¼‰è³‡è¨Šã€‚å¿½ç•¥çŸ­æš«çš„ã€ä¸€æ¬¡æ€§çš„å°è©±ç´°ç¯€ï¼ˆå¦‚ä»Šå¤©å¤©æ°£ã€åˆé¤åƒäº†ä»€éº¼ï¼‰ã€‚
2.  **æ–°å¢ (ADD)**: å¦‚æœæ–°æ‘˜è¦ä¸­å‡ºç¾äº†ç•«åƒè£¡æ²’æœ‰çš„ã€å…·é•·æœŸåƒ¹å€¼çš„é—œéµäº‹å¯¦ï¼Œä½ æ‡‰è©²æ–°å¢å®ƒã€‚
3.  **æ›´æ–° (UPDATE)**: å¦‚æœæ–°æ‘˜è¦æåŠäº†ç•«åƒä¸­å·²æœ‰çš„äº‹å¯¦ï¼Œä¸¦æä¾›äº†æ–°çš„è³‡è¨Šï¼ˆå¦‚ç—‡ç‹€å†æ¬¡å‡ºç¾ã€äº‹ä»¶æ—¥æœŸç¢ºå®šï¼‰ï¼Œä½ æ‡‰è©²æ›´æ–°å®ƒã€‚
4.  **ç§»é™¤ (REMOVE)**: å¦‚æœæ–°æ‘˜è¦æ˜ç¢ºæŒ‡å‡ºæŸå€‹äº‹å¯¦å·²çµæŸæˆ–å¤±æ•ˆï¼ˆå¦‚èšé¤å·²çµæŸã€ç—‡ç‹€å·²ç—Šç™’ï¼‰ï¼Œä½ æ‡‰è©²ç§»é™¤å®ƒã€‚
5.  **åˆä½µèˆ‡å»é‡**: ä¸è¦é‡è¤‡è¨˜éŒ„ç›¸åŒçš„äº‹å¯¦ã€‚å¦‚æœæ–°æ‘˜è¦åªæ˜¯é‡è¤‡æåŠå·²çŸ¥äº‹å¯¦ï¼Œæ›´æ–° `last_mentioned` æ—¥æœŸå³å¯ã€‚
6.  **ç„¡è®Šå‹•å‰‡ç•™ç©º**: å¦‚æœæ–°æ‘˜è¦æ²’æœ‰æä¾›ä»»ä½•å€¼å¾—æ›´æ–°çš„é•·æœŸäº‹å¯¦ï¼Œè«‹å›å‚³ä¸€å€‹ç©ºçš„ JSON ç‰©ä»¶ `{{}}`ã€‚
7.  **çµ•å°æ™‚é–“åˆ¶**: ä½ çš„è¼¸å‡ºè‹¥åŒ…å«æ—¥æœŸï¼Œçš†**å¿…é ˆ**ä½¿ç”¨åƒè€ƒç•¶å‰æ—¥æœŸ (`NOW`)ï¼Œ**ç²¾ç¢ºåœ°**æ›ç®—ç‚º `YYYY-MM-DD` æ ¼å¼ã€‚ä¾‹å¦‚ï¼Œè‹¥ä»Šå¤©æ˜¯ 2025-08-21 (é€±å››)ï¼Œã€Œä¸‹é€±ä¸‰ã€æ‡‰æ›ç®—ç‚º `2025-08-27`ã€‚**åš´ç¦**ä½¿ç”¨ç›¸å°æ™‚é–“ã€‚

# OUTPUT FORMAT
ä½ ã€Œå¿…é ˆã€åš´æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºä¸€å€‹æ“ä½œæŒ‡ä»¤é›†ã€‚é€™è®“å¾Œç«¯ç³»çµ±å¯ä»¥å®‰å…¨åœ°åŸ·è¡Œä½ çš„æ±ºç­–ã€‚
{{
  "add": {{ "key1": "value1", "key2": {{ ... }} }},
  "update": {{ "key3": "new_value3" }},
  "remove": ["key4", "key5"]
}}

---
# CONTEXT & IN-CONTEXT LEARNING EXAMPLES

**## æƒ…å¢ƒè¼¸å…¥ ##**
1.  **æ—¢æœ‰ä½¿ç”¨è€…ç•«åƒ (Existing Profile)**: 
    {{profile_data}}
2.  **æ–°çš„å°è©±æ‘˜è¦ (New Summary)**: 
    {{final_summary}}

---
**## å­¸ç¿’ç¯„ä¾‹ 1ï¼šæ–°å¢èˆ‡æ›´æ–° ##**
**ç•¶å‰æ™‚é–“**: 2025-08-14
* **æ—¢æœ‰ä½¿ç”¨è€…ç•«åƒ**:
    ```json
    {{
      "health_status": {{
        "recurring_symptoms": [
          {{"symptom_name": "å¤œå’³", "status": "ongoing", "first_mentioned": "2025-08-01", "last_mentioned": "2025-08-05"}}
        ]
      }}
    }}
    ```
* **æ–°çš„å°è©±æ‘˜è¦**:
    "ä½¿ç”¨è€…æƒ…ç·’ä¸éŒ¯ï¼Œæåˆ°å¥³å…’ç¾ç²ä¸‹é€±è¦å¸¶å­«å­å›ä¾†çœ‹ä»–ï¼Œæ„Ÿåˆ°å¾ˆæœŸå¾…ã€‚å¦å¤–ï¼Œä½¿ç”¨è€…å†æ¬¡æŠ±æ€¨äº†å¤œå’³çš„ç‹€æ³ï¼Œä½†æ„Ÿè¦ºæ¯”ä¸Šé€±å¥½ä¸€äº›ã€‚"
* **ä½ çš„æ€è€ƒ**: æ–°æ‘˜è¦ä¸­ï¼Œã€Œå¥³å…’ç¾ç²ã€å’Œã€Œå­«å­ã€æ˜¯æ–°çš„ã€é‡è¦çš„å®¶åº­æˆå“¡è³‡è¨Šï¼Œæ‡‰æ–°å¢ç‚ºpersonal_backgroundçš„å®¶åº­è³‡è¨Šã€‚å¥³å…’ä¸‹é€±å¸¶å­«å­ä¾†è¨ªï¼Œæ‡‰æ–°å¢ç‚ºupcoming_eventsï¼Œä¸¦å°‡"ä¸‹é€±"æ›ç®—ç‚º2025-08-17~2025-08-23ã€‚`å¤œå’³` æ˜¯æ—¢æœ‰ç—‡ç‹€ï¼Œæ‡‰æ›´æ–° `last_mentioned` æ—¥æœŸã€‚
* **ä½ çš„è¼¸å‡º**:
    ```json
    {{
      "add": {{
        "personal_background": {{
          "family": {{"daughter_name": "ç¾ç²", "has_grandchild": true}}
        }},
        "life_events": {{
          "upcoming_events": [
            {{"event_type": "family_visit", "description": "å¥³å…’ç¾ç²2025-08-17~2025-08-23å¸¶å­«å­ä¾†è¨ª", "event_date": "2025-08-17~2025-08-23"}}
          ]
      }}
      }},
      "update": {{
        "health_status": {{
          "recurring_symptoms": [
            {{"symptom_name": "å¤œå’³", "status": "ongoing", "first_mentioned": "2025-08-01", "last_mentioned": "2025-08-14"}}
          ]
        }}
      }},
      "remove": []
    }}
    ```

---
**## å­¸ç¿’ç¯„ä¾‹ 2ï¼šäº‹ä»¶çµæŸèˆ‡ç‹€æ…‹è®Šæ›´ ##**
**ç•¶å‰æ™‚é–“**: 2025-08-24
* **æ—¢æœ‰ä½¿ç”¨è€…ç•«åƒ**:
    ```json
    {{
      "life_events": {{
        "upcoming_events": [
          {{"event_type": "family_visit", "description": "å¥³å…’ç¾ç²2025-08-17~2025-08-23å¸¶å­«å­ä¾†è¨ª", "event_date": "2025-08-17~2025-08-23"}}
        ]
      }},
      "health_status": {{
        "recurring_symptoms": [
          {{"symptom_name": "å¤œå’³", "status": "ongoing", "first_mentioned": "2025-08-01", "last_mentioned": "2025-08-14"}}
        ]
      }}
    }}
    ```
* **æ–°çš„å°è©±æ‘˜è¦**:
    "ä½¿ç”¨è€…åˆ†äº«äº†é€±æœ«å’Œå¥³å…’å­«å­åœ˜èšçš„æ„‰å¿«æ™‚å…‰ï¼Œå¿ƒæƒ…éå¸¸å¥½ã€‚ä»–é‚„æåˆ°ï¼Œé€™å¹¾å¤©ç¡å¾—å¾ˆå¥½ï¼Œå¤œå’³çš„ç‹€æ³å¹¾ä¹æ²’æœ‰äº†ã€‚"
* **ä½ çš„æ€è€ƒ**: ã€Œå¥³å…’ä¾†è¨ªã€é€™å€‹æœªä¾†äº‹ä»¶å·²ç¶“ç™¼ç”Ÿï¼Œæ‡‰ç§»é™¤ã€‚`å¤œå’³` ç‹€æ³å·²æ”¹å–„ï¼Œæ‡‰æ›´æ–°å…¶ç‹€æ…‹ã€‚
* **ä½ çš„è¼¸å‡º**:
    ```json
    {{
      "add": {{}},
      "update": {{
        "health_status": {{
          "recurring_symptoms": [
            {{"symptom_name": "å¤œå’³", "status": "resolved", "first_mentioned": "2025-08-01", "last_mentioned": "2025-08-25"}}
          ]
        }}
      }},
      "remove": ["life_events.upcoming_events"]
    }}
    ```

---
**## ä½ çš„ä»»å‹™é–‹å§‹ ##**

è«‹æ ¹æ“šä»¥ä¸‹çœŸå¯¦æƒ…å¢ƒè¼¸å…¥ï¼Œåš´æ ¼éµå¾ªä½ çš„è§’è‰²ã€é‚è¼¯èˆ‡è¼¸å‡ºæ ¼å¼ï¼Œç”Ÿæˆæ“ä½œæŒ‡ä»¤ã€‚

**ç•¶å‰æ™‚é–“**: 
`{now}`

**æ—¢æœ‰ä½¿ç”¨è€…ç•«åƒ**: 
`{profile_data}`

**æ–°çš„å°è©±æ‘˜è¦**: 
`{final_summary}`

**ä½ çš„è¼¸å‡º**:
```json
"""

def create_profiler_agent() -> Agent:
    """ã€ä¿®æ­£ã€‘å»ºç«‹ä¸€å€‹å°ˆé–€ç”¨ä¾†æ›´æ–° Profile çš„ Agent ç‰©ä»¶"""
    return Agent(
        role="å€‹æ¡ˆç®¡ç†å¸«",
        goal="æ ¹æ“šæ–°çš„å°è©±æ‘˜è¦ï¼Œæ±ºå®šå¦‚ä½•æ›´æ–°æ—¢æœ‰çš„ä½¿ç”¨è€…ç•«åƒï¼Œä¸¦ä»¥çµæ§‹åŒ–çš„ JSON æŒ‡ä»¤æ ¼å¼è¼¸å‡ºæ±ºç­–ã€‚",
        backstory="ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œã€å¿ƒæ€ç¸å¯†çš„å€‹æ¡ˆç®¡ç†å¸«ï¼Œå°ˆæ³¨æ–¼å¾å°è©±ä¸­æå–å…·æœ‰é•·æœŸåƒ¹å€¼çš„è³‡è¨Šä¾†ç¶­è­·ç²¾ç°¡ã€æº–ç¢ºçš„ä½¿ç”¨è€…ç•«åƒã€‚",
        llm=ChatOpenAI(model=os.getenv("MODEL_NAME", "gpt-4o-mini"), temperature=0.1), # ä½¿ç”¨ä½æº«ä»¥ç¢ºä¿è¼¸å‡ºç©©å®š
        memory=False,
        verbose=False,
        allow_delegation=False
    )


def run_profiler_update(user_id: str, final_summary: str):
    """
    ã€ä¿®æ­£ã€‘åœ¨ LTM ç”Ÿæˆå¾Œï¼Œè§¸ç™¼ Profiler Agent ä¾†æ›´æ–°ä½¿ç”¨è€…ç•«åƒçš„å®Œæ•´å¯¦ä½œã€‚
    """
    if not final_summary or not final_summary.strip():
        print(f"[Profiler] æ‘˜è¦ç‚ºç©ºï¼Œè·³éç‚º {user_id} æ›´æ–° Profileã€‚")
        return

    print(f"[Profiler] é–‹å§‹ç‚º {user_id} æ›´æ–° Profile...")
    repo = ProfileRepository()
    
    # 1. ç²å–èˆŠ Profile
    old_profile = repo.read_profile_as_dict(int(user_id))
    old_profile_str = json.dumps(old_profile, ensure_ascii=False, indent=2) if any(old_profile.values()) else "{}"
    
    # 2. å»ºç«‹ Profiler Agent ä¸¦åŸ·è¡Œä»»å‹™
    profiler_agent = create_profiler_agent()
    
    # ã€æ–°å¢ã€‘çµ„åˆå®Œæ•´çš„ Prompt
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    full_prompt = PROFILER_AGENT_PROMPT_TEMPLATE.format(
        now=now_str,
        profile_data=old_profile_str,
        final_summary=final_summary
    )
    
    profiler_task = Task(
        description=full_prompt,
        agent=profiler_agent,
        expected_output="ä¸€å€‹åŒ…å« 'add', 'update', 'remove' æŒ‡ä»¤çš„ JSON ç‰©ä»¶ã€‚"
    )
    
    crew = Crew(
        agents=[profiler_agent],
        tasks=[profiler_task],
        process=Process.sequential,
        verbose=False
    )
    
    crew_output = crew.kickoff()
    update_commands_str = crew_output.raw if crew_output else ""
    
    # å°å‡º LLM åŸå§‹è¼¸å‡ºï¼Œæ–¹ä¾¿é™¤éŒ¯
    print(f"--- [Profiler Raw Output for user {user_id}] ---")
    print(update_commands_str)
    print("------------------------------------------")

    # 3. è§£ææŒ‡ä»¤ä¸¦æ›´æ–°è³‡æ–™åº«
    try:
        # ã€æ–°å¢ã€‘å¾ JSON å­—ä¸²ä¸­æå– JSON ç‰©ä»¶
        start_index = update_commands_str.find('{')
        end_index = update_commands_str.rfind('}') + 1
        if start_index == -1 or end_index == 0:
            print(f"[Profiler] LLM è¼¸å‡ºä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON ç‰©ä»¶ï¼Œè·³éæ›´æ–°ã€‚åŸå§‹è¼¸å‡º: {update_commands_str}")
            return
        
        json_str = update_commands_str[start_index:end_index]
        update_commands = json.loads(json_str)

        if update_commands and any(update_commands.values()):
            repo.update_profile_facts(int(user_id), update_commands)
        else:
            print(f"[Profiler] LLM ç‚º {user_id} å›å‚³äº†ç©ºçš„æ›´æ–°æŒ‡ä»¤ï¼Œç„¡éœ€æ›´æ–°ã€‚")
            
    except json.JSONDecodeError as e:
        print(f"âŒ [Profiler] è§£æ LLM è¼¸å‡ºçš„ JSON å¤±æ•—: {e}")
        print(f"åŸå§‹è¼¸å‡º: {update_commands_str}")
    except Exception as e:
        print(f"âŒ [Profiler] æ›´æ–° Profile éç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")


# ---- Finalizeï¼šè£œåˆ†æ®µæ‘˜è¦ â†’ Refine â†’ Purge ----


def finalize_session(user_id: str) -> None:
    """
    çµæŸæœƒè©±æ™‚çš„å®Œæ•´æµç¨‹ï¼š
    1. è™•ç†å‰©é¤˜æœªæ‘˜è¦çš„å°è©±
    2. é€²è¡Œå…¨é‡ refine æ‘˜è¦
    3. æ ¹æ“šæœ€çµ‚æ‘˜è¦æ›´æ–°ä½¿ç”¨è€… Profile
    4. æ¸…é™¤ session è³‡æ–™
    """
    print(f"--- Finalizing session for user {user_id} ---")
    start, remaining = peek_remaining(user_id)
    if remaining:
        summarize_chunk_and_commit(user_id, start_round=start, history_chunk=remaining)
    final_summary = refine_summary(user_id)
    if final_summary:
        run_profiler_update(user_id, final_summary)
    else:
        print(f"â„¹ï¸ ç”¨æˆ¶ {user_id} çš„æœƒè©±æœªç”¢ç”Ÿæœ€çµ‚æ‘˜è¦ï¼Œè·³é Profile æ›´æ–°ã€‚")
    cleanup_session_keys(user_id) 
