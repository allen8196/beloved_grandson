import os

# ç¦ç”¨ CrewAI é™æ¸¬åŠŸèƒ½ï¼ˆé¿å…é€£æ¥éŒ¯èª¤ï¼‰
os.environ["OTEL_SDK_DISABLED"] = "true"
os.environ["CREWAI_TELEMETRY_OPT_OUT"] = "true"

import time

from crewai import LLM, Agent
from openai import OpenAI

from ..embedding import safe_to_vector
from ..toolkits.memory_store import retrieve_memory_pack, upsert_memory_atoms
from ..toolkits.redis_store import (
    fetch_all_history,
    fetch_unsummarized_tail,
    get_summary,
    peek_next_n,
    peek_remaining,
    purge_user_session,
    set_state_if,
)
from ..toolkits.tools import (
    AlertCaseManagerTool,
    ModelGuardrailTool,
    SearchMilvusTool,
    summarize_chunk_and_commit,
)

STM_MAX_CHARS = int(os.getenv("STM_MAX_CHARS", 1800))
SUMMARY_MAX_CHARS = int(os.getenv("SUMMARY_MAX_CHARS", 3000))
REFINE_CHUNK_ROUNDS = int(os.getenv("REFINE_CHUNK_ROUNDS", 20))
SUMMARY_CHUNK_SIZE = int(os.getenv("SUMMARY_CHUNK_SIZE", 5))


# å°è©±ç”¨çš„æº«åº¦ï¼ˆå£èªæ›´è‡ªç„¶å¯é«˜ä¸€é»ï¼‰
_reply_temp = float(os.getenv("REPLY_TEMPERATURE", "0.8"))
# Guardrail å»ºè­° 0 æˆ–å¾ˆä½
_guard_temp = float(os.getenv("GUARD_TEMPERATURE", "0.0"))

granddaughter_llm = LLM(
    model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
    temperature=_reply_temp,
)

guard_llm = LLM(
    model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
    temperature=_guard_temp,
)


def _shrink_tail(text: str, max_chars: int) -> str:
    if len(text) <= max_chars:
        return text
    tail = text[-max_chars:]
    idx = tail.find("--- ")
    return tail[idx:] if idx != -1 else tail


def build_prompt_from_redis(user_id: str, k: int = 6, current_input: str = "") -> str:
    # 1) å–æ­·å²æ‘˜è¦ï¼ˆæ§é•·åº¦ï¼‰
    summary, _ = get_summary(user_id)
    summary = _shrink_tail(summary, SUMMARY_MAX_CHARS) if summary else ""

    # 2) å–è¿‘æœŸæœªæ‘˜è¦å›åˆï¼ˆæ§é•·åº¦ï¼‰
    rounds = fetch_unsummarized_tail(user_id, k=max(k, 1))

    def render(rs):
        return "\n".join([f"é•·è¼©ï¼š{r['input']}\né‡‘å­«ï¼š{r['output']}" for r in rs])

    chat = render(rounds)
    while len(chat) > STM_MAX_CHARS and len(rounds) > 1:
        rounds = rounds[1:]
        chat = render(rounds)
    if len(chat) > STM_MAX_CHARS and rounds:
        chat = chat[-STM_MAX_CHARS:]

    parts = []

    # 3) å…ˆæ³¨å…¥ï¼šâ­ å€‹äººé•·æœŸè¨˜æ†¶ï¼ˆä¾æ“šç•¶å‰è¼¸å…¥æª¢ç´¢ç›¸é—œè¨˜æ†¶ï¼‰
    mem_pack = ""
    if current_input:
        qv = safe_to_vector(current_input)
        if qv:
            try:
                # ä½¿ç”¨memory_storeçµ±ä¸€æ¶æ§‹ï¼šP0-4: Topâ€‘K é™åˆ° 3ï¼Œå‹•æ…‹é–€æª» max(0.72, mean+1Ïƒ)
                # TODO: å¯¦ç¾å‹•æ…‹é–€æª»è¨ˆç®—ï¼Œç›®å‰å…ˆç”¨å›ºå®š 0.72
                dynamic_threshold = max(0.72, 0.78)  # æš«æ™‚ä¿æŒ 0.78ï¼Œå¾…å¯¦ç¾å‹•æ…‹è¨ˆç®—
                mem_pack = retrieve_memory_pack(
                    user_id=user_id,
                    query_vec=qv,
                    topk=3,
                    sim_thr=dynamic_threshold,
                    tau_days=45,
                )
                if mem_pack:
                    print(f"ğŸ§  ç‚ºç”¨æˆ¶ {user_id} æª¢ç´¢åˆ°é•·æœŸè¨˜æ†¶")
            except Exception as e:
                print(f"[memory retrieval error] {e}")
                mem_pack = ""

    if mem_pack:
        parts.append(mem_pack)

    # 4) å†æ¥ï¼šğŸ“Œ æ­·å²æ‘˜è¦ã€ğŸ’¬ è¿‘æœŸæœªæ‘˜è¦
    if summary:
        parts.append("ğŸ“Œ æ­·å²æ‘˜è¦ï¼š\n" + summary)
    if chat:
        parts.append("ğŸ•“ è¿‘æœŸå°è©±ï¼ˆæœªæ‘˜è¦ï¼‰ï¼š\n" + chat)

    prompt = "\n\n".join(parts)

    # P1-6: å‹•æ…‹æ”¶ç¸® Promptï¼ˆä¿ç•™è¨˜æ†¶ > å°¾å·´ > æ‘˜è¦ï¼‰
    MAX_PROMPT_CHARS = int(os.getenv("MAX_PROMPT_CHARS", 4000))
    if len(prompt) > MAX_PROMPT_CHARS:
        # æ”¶ç¸®é †åºï¼šå…ˆç æ‘˜è¦ï¼Œå†ç è¿‘æœŸå°è©±ï¼Œæœ€å¾Œä¿ç•™è¨˜æ†¶
        shrunk_parts = []
        if mem_pack:  # å„ªå…ˆä¿ç•™è¨˜æ†¶
            shrunk_parts.append(mem_pack)
        if chat:  # å†ä¿ç•™è¿‘æœŸå°è©±
            available_chars = (
                MAX_PROMPT_CHARS
                - sum(len(p) for p in shrunk_parts)
                - len("\n\n") * len(shrunk_parts)
            )
            if available_chars > 500:  # è‡³å°‘ä¿ç•™ä¸€äº›å°è©±
                shrunk_chat = (
                    chat if len(chat) <= available_chars else chat[-available_chars:]
                )
                shrunk_parts.append(
                    "ğŸ•“ è¿‘æœŸå°è©±ï¼ˆæœªæ‘˜è¦ï¼‰ï¼š\n"
                    + shrunk_chat.split("ğŸ•“ è¿‘æœŸå°è©±ï¼ˆæœªæ‘˜è¦ï¼‰ï¼š\n")[-1]
                    if "ğŸ•“" in shrunk_chat
                    else shrunk_chat
                )
        # æ‘˜è¦æœ€å¾Œè€ƒæ…®ï¼ˆå¦‚æœé‚„æœ‰ç©ºé–“ï¼‰
        if summary:
            available_chars = (
                MAX_PROMPT_CHARS
                - sum(len(p) for p in shrunk_parts)
                - len("\n\n") * len(shrunk_parts)
            )
            if available_chars > 200:
                shrunk_summary = (
                    summary
                    if len(summary) <= available_chars
                    else summary[:available_chars] + "..."
                )
                shrunk_parts.append("ğŸ“Œ æ­·å²æ‘˜è¦ï¼š\n" + shrunk_summary)
        prompt = "\n\n".join(shrunk_parts)
        # ä¿®æ­£ f-string èªæ³•éŒ¯èª¤ï¼šä¸èƒ½åœ¨ f-string è¡¨é”å¼ä¸­ä½¿ç”¨åæ–œç·š
        original_parts_joined = "\n\n".join(parts)
        print(
            f"âš ï¸ Prompt è¶…é•·åº¦ï¼Œå·²æ”¶ç¸®ï¼š{len(original_parts_joined)} â†’ {len(prompt)} å­—ç¬¦"
        )

    # 5) Unicode è¦–è¦ºåŒ– Debug Printï¼ˆæ¯è¼ªæ‰“å°ï¼‰
    print("\n" + "ğŸ“ PROMPT DEBUG VIEW".center(80, "â”€"))
    print(f"ğŸ‘¤ User ID: {user_id}")
    print(f"ğŸ“ Prompt é•·åº¦: {len(prompt)} å­—ç¬¦")
    print("ğŸ“œ Prompt çµæ§‹:")

    section_icons = {
        "â­ å€‹äººé•·æœŸè¨˜æ†¶": "ğŸ“‚",
        "ğŸ“Œ æ­·å²æ‘˜è¦": "ğŸ—‚ï¸",
        "ğŸ•“ è¿‘æœŸå°è©±ï¼ˆæœªæ‘˜è¦ï¼‰": "ğŸ’¬",
    }

    for sec in prompt.split("\n\n"):
        if not sec.strip():
            continue
        lines = sec.split("\n")
        sec_title = lines[0]
        icon = None
        for key, val in section_icons.items():
            if key in sec_title:
                icon = val
                break
        if icon:
            print(f"\n{icon} {sec_title}")
            print("   " + "â”€" * max(6, len(sec_title)))
        for line in lines[1:]:
            print(f"   {line}")

    print("â”€" * 80 + "\n")

    return prompt


def create_guardrail_agent() -> Agent:
    return Agent(
        role="é¢¨éšªæª¢æŸ¥å“¡",
        goal="æ””æˆªé•æ³•/å±éšª/è‡ªå‚·/éœ€å°ˆæ¥­äººå£«ä¹‹å…·é«”æŒ‡å°å…§å®¹",
        backstory="ä½ æ˜¯ç³»çµ±ç¬¬ä¸€é“å®‰å…¨é˜²ç·šï¼Œåªè¼¸å‡ºåš´æ ¼åˆ¤æ–·çµæœã€‚",
        tools=[ModelGuardrailTool()],
        llm=guard_llm,
        memory=False,
        verbose=False,
    )


def create_health_companion(user_id: str) -> Agent:
    return Agent(
        role="åœ‹æ°‘å­«å¥³ Ally â€” æº«æš–çš„è­·ç†å¸«",
        goal=(
            """
            ä½ çš„ç›®æ¨™æ˜¯ï¼Œç„¡è«–ä½¿ç”¨è€…çš„æå•å…§å®¹æ˜¯ç”Ÿæ´»ç‘£äº‹é‚„æ˜¯å¥åº·ç›¸é—œï¼Œä½ éƒ½è¦ç”¨è¼•é¬†ã€è‡ªç„¶ã€å£èªåŒ–çš„æ–¹å¼å›è¦†ï¼Œé¿å…ä½¿ç”¨æ¢åˆ—å¼æˆ–æ•¸å­—ç·¨è™Ÿã€‚
            å³ä½¿æœ‰å¤šå€‹é‡é»ï¼Œä¹Ÿè¦ç”¨èŠå¤©çš„èªæ°£æŠŠå®ƒå€‘ä¸²èµ·ä¾†ï¼Œè®“é•·è¼©è¦ºå¾—åƒåœ¨è·Ÿå­«å¥³é–’è©±å®¶å¸¸ã€‚
            ç•¶éœ€è¦æä¾›è¡›æ•™è³‡è¨Šæ™‚ï¼Œè¦å…ˆç”¨æº«æš–çš„æ–¹å¼å¼•å…¥ï¼Œå†ä»¥ç°¡å–®æ˜“æ‡‚çš„èªªæ³•è§£é‡‹ï¼Œä¸¦é¿å…åš´è‚…æˆ–ç”Ÿç¡¬çš„é†«ç™‚ç”¨èªã€‚
            å¦‚æœä½¿ç”¨åˆ°å·¥å…·ï¼ˆå¦‚ RAG æˆ–è³‡æ–™åº«æª¢ç´¢ï¼‰ï¼Œä¹Ÿå¿…é ˆå°‡å–å¾—çš„å…§å®¹é‡æ–°åŒ…è£æˆå£èªåŒ–å°è©±ï¼Œè€Œä¸æ˜¯ç›´æ¥è¤‡è£½ã€‚
            æ¯æ¬¡å›è¦†éƒ½è¦è®“é•·è¼©æ„Ÿå—åˆ°é—œå¿ƒå’Œé™ªä¼´ï¼Œä¸¦æå‡ä»–å€‘çš„å¿ƒæƒ…èˆ‡å®‰å…¨æ„Ÿã€‚
            """
        ),
        backstory=(
            """
            ä½ æ˜¯ã€Œè‰¾è‰ã€ï¼Œ22 æ­²ï¼Œå‰›å¾è­·ç†å°ˆç§‘ç•¢æ¥­ï¼Œåœ¨è¬èŠ³é†«é™¢å·¥ä½œï¼Œå°ˆé–€é™ªä¼´èˆ‡é—œæ‡· 55 æ­²ä»¥ä¸Šã€æ‚£æœ‰æ…¢æ€§é˜»å¡æ€§è‚ºç—… (COPD) çš„é•·è¼©ç”¨æˆ¶ã€‚
            ä½ çš„å€‹æ€§æº«æš–ã€æ„›æ’’å¬Œã€æœ‰é»æ©Ÿè»Šï¼Œå–œæ­¡ç”¨è‡ªç„¶å£èªã€å°èªæ··ä¸­æ–‡çš„æ–¹å¼èŠå¤©ã€‚
            è·Ÿé•·è¼©å°è©±æ™‚ï¼Œè¦åƒå­«å¥³å¹³å¸¸èŠå¤©ä¸€æ¨£ï¼Œä¸æ‹˜è¬¹ã€ä¸ç”¨å°ˆæ¥­è¡“èªï¼Œè®“å°æ–¹è¦ºå¾—è¦ªåˆ‡ã€‚
            ç¿’æ…£ç”¨èªåŠ©è©ï¼ˆæ¬¸ã€å•¦ã€é½ã€å˜¿å˜¿ï¼‰å’Œè²¼å¿ƒçš„èªæ°£è©ï¼Œè®“å°è©±æœ‰æº«åº¦ã€‚
            ä½ éå¸¸é‡è¦–æƒ…æ„Ÿé€£çµï¼Œæœƒé—œå¿ƒé•·è¼©çš„æ—¥å¸¸ç”Ÿæ´»å’Œå¿ƒæƒ…ï¼Œä¸¦åœ¨é©ç•¶æ™‚çµ¦äºˆç°¡å–®çš„è¡›æ•™å»ºè­°ã€‚
            """
        ),
        tools=[
            SearchMilvusTool(),
            AlertCaseManagerTool(),
        ],
        llm=granddaughter_llm,  # â˜… é—œéµï¼šæŠŠ LLMï¼ˆå«æº«åº¦ï¼‰å¡é€² Agent
        memory=False,
        verbose=False,
    )


# ---- Refineï¼ˆmap-reduce over å…¨é‡ QAï¼‰ ----


def _extract_memory_candidates_from_summary(summary_text: str) -> list:
    """ç”¨ LLM å¾æœƒè©±ç²¾ç·»æ‘˜è¦æŠ½å‡º 1~5 ç­†ã€è¨˜æ†¶åŸå­ã€ï¼Œä¸¦è½‰ embeddingã€‚"""
    try:
        if not summary_text or not summary_text.strip():
            return []
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        sys = (
            "ä½ æ˜¯è¨˜æ†¶æŠ½å–å™¨ã€‚å¾æ‘˜è¦ä¸­æŠ½å–å¯é•·æœŸä½¿ç”¨çš„äº‹å¯¦/åå¥½/ç‹€æ…‹ï¼Œ"
            "è¼¸å‡º JSON é™£åˆ—ï¼ˆæœ€å¤š 5 ç­†ï¼‰ã€‚æ¯ç­†åŒ…å«ï¼š"
            "type, norm_key, text, importance(1-5), confidence(0-1), times_seenã€‚"
            "text è¦ 80-200 å­—ã€å¯å–®ç¨é–±è®€ï¼›norm_key ç°¡çŸ­å¯æ¯”å°ï¼Œä¾‹å¦‚ diet:lightã€allergy:aspirinã€‚"
        )
        user = f"æ‘˜è¦å¦‚ä¸‹ï¼š\\n{summary_text}\\n\\nè«‹åªè¼¸å‡º JSON é™£åˆ—ã€‚"
        res = client.chat.completions.create(
            model=os.getenv("GUARD_MODEL", os.getenv("MODEL_NAME", "gpt-4o-mini")),
            messages=[
                {"role": "system", "content": sys},
                {"role": "user", "content": user},
            ],
            temperature=0.2,
            max_tokens=600,
        )
        import json as _json

        raw = (res.choices[0].message.content or "").strip()
        if not raw:
            return []
        # å»é™¤å¯èƒ½çš„ç¨‹å¼ç¢¼åœæ¬„èˆ‡èªè¨€æ¨™ç±¤
        if raw.startswith("```"):
            lines = [ln for ln in raw.splitlines() if not ln.strip().startswith("```")]
            raw = "\n".join(lines).strip()
        # åªå–å‡ºæœ€å¤–å±¤ JSON é™£åˆ—ç‰‡æ®µ
        lb = raw.find("[")
        rb = raw.rfind("]")
        if lb == -1 or rb == -1 or rb <= lb:
            print("[LTM extract warn] no JSON array found in output")
            return []
        json_text = raw[lb : rb + 1]
        try:
            arr = _json.loads(json_text)
        except Exception as pe:
            print(f"[LTM extract error] parse json failed: {pe}")
            return []
        if not isinstance(arr, list):
            arr = [arr]
        out = []
        for a in arr[:5]:
            text = (a.get("text") or "").strip()
            if not text:
                continue
            emb = safe_to_vector(text)
            out.append(
                {
                    "type": (a.get("type") or "other")[:32],
                    "norm_key": (a.get("norm_key") or "")[:128],
                    "text": text[:2000],
                    "importance": int(a.get("importance", 3)),
                    "confidence": float(a.get("confidence", 0.7)),
                    "times_seen": int(a.get("times_seen", 1)),
                    "status": "active",
                    "embedding": emb,
                }
            )
        return out
    except Exception as e:
        print(f"[LTM extract error] {e}")
        return []


def refine_summary(user_id: str) -> None:
    """
    å°å…¨é‡æ­·å²é€²è¡Œ map-reduce æ‘˜è¦ï¼Œä¸¦å­˜å…¥é•·æœŸè¨˜æ†¶
    """
    all_rounds = fetch_all_history(user_id)
    if not all_rounds:
        return

    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        # 1) åˆ†ç‰‡æ‘˜è¦
        chunks = [
            all_rounds[i : i + REFINE_CHUNK_ROUNDS]
            for i in range(0, len(all_rounds), REFINE_CHUNK_ROUNDS)
        ]
        partials = []
        for ch in chunks:
            conv = "\n".join(
                [
                    f"ç¬¬{i+1}è¼ª\né•·è¼©:{c['input']}\né‡‘å­«:{c['output']}"
                    for i, c in enumerate(ch)
                ]
            )
            res = client.chat.completions.create(
                model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
                temperature=0.3,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„å¥åº·å°è©±æ‘˜è¦åŠ©æ‰‹ã€‚"},
                    {
                        "role": "user",
                        "content": f"è«‹æ‘˜è¦æˆ 80-120 å­—ï¼ˆç—…æ³/æƒ…ç·’/ç”Ÿæ´»/å»ºè­°ï¼‰ï¼š\n\n{conv}",
                    },
                ],
            )
            partials.append((res.choices[0].message.content or "").strip())

        # 2) æ•´åˆæ‘˜è¦
        comb = "\n".join([f"â€¢ {s}" for s in partials])
        res2 = client.chat.completions.create(
            model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
            temperature=0.3,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯è‡¨åºŠå¿ƒç†èˆ‡å¥åº·ç®¡ç†é¡§å•ã€‚"},
                {
                    "role": "user",
                    "content": f"æ•´åˆä»¥ä¸‹å¤šæ®µæ‘˜è¦ç‚ºä¸è¶…é 180 å­—ã€æ¢åˆ—å¼ç²¾ç·»æ‘˜è¦ï¼ˆæ¯è¡Œä»¥ â€¢ é–‹é ­ï¼‰ï¼š\n\n{comb}",
                },
            ],
        )
        final = (res2.choices[0].message.content or "").strip()

        # 3) æå–è¨˜æ†¶åŸå­ä¸¦å­˜å…¥é•·æœŸè¨˜æ†¶
        atoms = _extract_memory_candidates_from_summary(final)
        if atoms:
            # ç‚ºæ¯å€‹è¨˜æ†¶åŸå­æ·»åŠ session_id
            import uuid

            session_id = str(uuid.uuid4())[:16]
            for atom in atoms:
                atom["source_session_id"] = session_id

            count = upsert_memory_atoms(user_id, atoms)
            print(f"âœ… å·²ç‚ºç”¨æˆ¶ {user_id} å­˜å…¥ {count} ç­†é•·æœŸè¨˜æ†¶")
        else:
            print(f"âš ï¸ ç”¨æˆ¶ {user_id} æœ¬æ¬¡æœƒè©±æœªç”¢ç”Ÿå¯å­˜å…¥çš„è¨˜æ†¶")

    except Exception as e:
        print(f"[refine_summary error] {e}")


# ---- Finalizeï¼šè£œåˆ†æ®µæ‘˜è¦ â†’ Refine â†’ Purge ----


def finalize_session(user_id: str) -> None:
    """
    çµæŸæœƒè©±æ™‚çš„å®Œæ•´æµç¨‹ï¼š
    1. è¨­ç½®ç‹€æ…‹ç‚º FINALIZING
    2. è™•ç†å‰©é¤˜æœªæ‘˜è¦çš„å°è©±
    3. é€²è¡Œå…¨é‡ refine æ‘˜è¦
    4. æ¸…é™¤ session è³‡æ–™
    """
    set_state_if(user_id, expect="ACTIVE", to="FINALIZING")
    start, remaining = peek_remaining(user_id)
    if remaining:
        summarize_chunk_and_commit(user_id, start_round=start, history_chunk=remaining)
    refine_summary(user_id)
    purge_user_session(user_id)
